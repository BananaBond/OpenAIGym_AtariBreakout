{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c881f145",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from rl_glue import RLGlue\n",
    "\n",
    "from environment import BaseEnvironment\n",
    "from atari_breakout import AtariBreakoutEnvironment\n",
    "from agent import BaseAgent \n",
    "\n",
    "from collections import deque\n",
    "from copy import deepcopy\n",
    "from tqdm import tqdm\n",
    "import os \n",
    "import time\n",
    "import shutil\n",
    "from plot_script import plot_result\n",
    "import pickle\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential, model_from_json\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a5ddb28",
   "metadata": {},
   "source": [
    "## Neural Network for action values\n",
    "\n",
    "We use a neural network with one hidden layer for approximating the action-value function in a control problem. The output layer size is the number of actions. \n",
    "\n",
    "The get_action_values() function computes the action-value function by doing a forward pass.\n",
    "The get_TD_update() function computes the gradient of the action-value function with respect to the weights times the TD error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "42e83475",
   "metadata": {},
   "outputs": [],
   "source": [
    "NAME_Save = \"Breakout-CNN-C-32-8-4__Mpad__C-64-4-3__Mpad__C-64-3-1__Mpad__D-512-{}\".format(int(time.time()))\n",
    "NAME_Load = \"\"\n",
    "class ActionValueNetwork:\n",
    "\n",
    "    def __init__(self, network_config):\n",
    "        self.state_dim = network_config.get(\"state_dim\")\n",
    "        self.num_actions = network_config.get(\"num_actions\")\n",
    "        self.learning_rate = network_config.get(\"learning_rate\")\n",
    "        \n",
    "#         inputs = layers.Input(shape=(self.state_dim, self.state_dim, 3))\n",
    "#         # Convolutions on the frames on the screen\n",
    "#         layer1 = layers.Conv2D(6, 7, strides=4, activation=\"relu\")(inputs)\n",
    "#         layer2 = layers.MaxPooling2D(pool_size = (2,2))(layer1)\n",
    "#         layer3 = layers.Conv2D(12, 4, strides=2, activation=\"relu\")(layer2)\n",
    "#         layer4 = layers.MaxPooling2D(pool_size = (2,2))(layer3)\n",
    "#         layer5 = layers.Flatten()(layer4)\n",
    "#         layer6 = layers.Dense(512, activation=\"relu\")(layer5)\n",
    "#         action = layers.Dense(self.num_actions, activation=\"linear\")(layer6)\n",
    "\n",
    "#         self.model =  keras.Model(inputs=inputs, outputs=action)\n",
    "        \n",
    "        model = Sequential()\n",
    "#         model.add(layers.Input(shape=(self.state_dim, self.state_dim, 3)))\n",
    "        model.add(layers.Conv2D(filters = 32, kernel_size = 8, strides = 4, activation=\"relu\", input_shape = (self.state_dim[0], self.state_dim[1], self.state_dim[2])))\n",
    "        \n",
    "        model.add(layers.MaxPooling2D(pool_size = (2,2), padding = 'same'))\n",
    "        model.add(layers.Conv2D(filters = 64, kernel_size = 4, strides = 3, activation=\"relu\"))\n",
    "        model.add(layers.MaxPooling2D(pool_size = (2,2), padding = 'same'))\n",
    "        model.add(layers.Conv2D(filters = 64, kernel_size = 3, strides = 1, activation=\"relu\"))\n",
    "        model.add(layers.MaxPooling2D(pool_size = (2,2), padding = 'same'))\n",
    "        model.add(layers.Flatten())\n",
    "        model.add(layers.Dense(512, activation=\"relu\"))\n",
    "        model.add(layers.Dense(self.num_actions, activation=None))\n",
    "              \n",
    "        model.compile(loss = 'mean_squared_error', optimizer=Adam(lr=self.learning_rate, epsilon=1e-7))\n",
    "        self.model = model\n",
    "#         model.summary()\n",
    "#         model = models.Sequential()\n",
    "#         model.add(layers.Conv2D(32, 8, strides = 4, activation='relu', input_shape=(32, 32, 3)))\n",
    "#         model.add(layers.MaxPooling2D((2, 2)))\n",
    "#         model.add(layers.Conv2D(64, 4, strides = 2, activation='relu'))\n",
    "#         model.add(layers.MaxPooling2D((2, 2)))\n",
    "#         model.add(layers.Conv2D(64, 3, activation='relu'))\n",
    "#         model.add(layers.Flatten())\n",
    "#         model.add(layers.Dense(512, activation=\"relu\")(layer4))\n",
    "#         model.add(layers.Dense(self.num_actions, activation=\"linear\"))\n",
    "        \n",
    "#         self.model = model\n",
    "\n",
    "    def get_action_values(self, s):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            s (Numpy array): The state.\n",
    "        Returns:\n",
    "            The action-values (Numpy array) calculated using the network's weights.\n",
    "        \"\"\"\n",
    "        \n",
    "        q_vals = self.model.predict(s)\n",
    "\n",
    "        return q_vals\n",
    "    \n",
    "\n",
    "#     def get_TD_update(self, s, delta_mat):\n",
    "#         \"\"\"\n",
    "#         Args:\n",
    "#             s (Numpy array): The state.\n",
    "#             delta_mat (Numpy array): A 2D array of shape (batch_size, num_actions). Each row of delta_mat  \n",
    "#             correspond to one state in the batch. Each row has only one non-zero element \n",
    "#             which is the TD-error corresponding to the action taken.\n",
    "#         Returns:\n",
    "#             The TD update (Array of dictionaries with gradient times TD errors) for the network's weights\n",
    "#         \"\"\"\n",
    "\n",
    "#         W0, b0 = self.weights[0]['W'], self.weights[0]['b']\n",
    "#         W1, b1 = self.weights[1]['W'], self.weights[1]['b']\n",
    "        \n",
    "#         psi = np.dot(s, W0) + b0\n",
    "#         x = np.maximum(psi, 0)\n",
    "#         dx = (psi > 0).astype(float)\n",
    "\n",
    "#         # td_update has the same structure as self.weights, that is an array of dictionaries.\n",
    "#         # td_update[0][\"W\"], td_update[0][\"b\"], td_update[1][\"W\"], and td_update[1][\"b\"] have the same shape as \n",
    "#         # self.weights[0][\"W\"], self.weights[0][\"b\"], self.weights[1][\"W\"], and self.weights[1][\"b\"] respectively\n",
    "#         td_update = [dict() for i in range(len(self.weights))]\n",
    "         \n",
    "#         v = delta_mat\n",
    "#         td_update[1]['W'] = np.dot(x.T, v) * 1. / s.shape[0]\n",
    "#         td_update[1]['b'] = np.sum(v, axis=0, keepdims=True) * 1. / s.shape[0]\n",
    "        \n",
    "#         v = np.dot(v, W1.T) * dx\n",
    "#         td_update[0]['W'] = np.dot(s.T, v) * 1. / s.shape[0]\n",
    "#         td_update[0]['b'] = np.sum(v, axis=0, keepdims=True) * 1. / s.shape[0]\n",
    "                \n",
    "#         return td_update\n",
    "        \n",
    "    \n",
    "    def get_weights(self):\n",
    "        return self.model.get_weights()\n",
    "    \n",
    "    def set_weights(self, weights):\n",
    "        self.model.set_weights(weights)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db59e3e5",
   "metadata": {},
   "source": [
    "## Replay Buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1447670b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, size, minibatch_size, seed, dirc, loadBool):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            size (integer): The size of the replay buffer.              \n",
    "            minibatch_size (integer): The sample size.\n",
    "            seed (integer): The seed for the random number generator. \n",
    "        \"\"\"\n",
    "        self.buffer = []\n",
    "        self.minibatch_size = minibatch_size\n",
    "        self.rand_generator = np.random.RandomState(seed)\n",
    "        self.max_size = size\n",
    "        self.directory = dirc\n",
    "        if loadBool:\n",
    "            self.load()\n",
    "    def append(self, state, action, reward, terminal, next_state):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            state (Numpy array): The state.              \n",
    "            action (integer): The action.\n",
    "            reward (float): The reward.\n",
    "            terminal (integer): 1 if the next state is a terminal state and 0 otherwise.\n",
    "            next_state (Numpy array): The next state.           \n",
    "        \"\"\"\n",
    "        if len(self.buffer) == self.max_size:\n",
    "            del self.buffer[0]\n",
    "        state = state.astype('float32')\n",
    "        next_state = next_state.astype('float32')\n",
    "        self.buffer.append([state, action, reward, terminal, next_state])\n",
    "\n",
    "    def sample(self):\n",
    "        \"\"\"\n",
    "        Returns:\n",
    "            A list of transition tuples including state, action, reward, terinal, and next_state\n",
    "        \"\"\"\n",
    "        idxs = self.rand_generator.choice(np.arange(len(self.buffer)), size=self.minibatch_size)\n",
    "        return [self.buffer[idx] for idx in idxs]\n",
    "\n",
    "    def size(self):\n",
    "        return len(self.buffer)\n",
    "    \n",
    "    def save(self):\n",
    "        path = os.path.join(self.directory, \"replay_buffer_1.txt\")\n",
    "        with open(path, \"wb\") as fp:   #Pickling\n",
    "            pickle.dump(self.buffer, fp)\n",
    "        print(\"Replay Buffer saved : \" + str(len(self.buffer)) )\n",
    "        \n",
    "    def load(self):\n",
    "        try:\n",
    "            path = os.path.join(self.directory, \"replay_buffer_1.txt\")\n",
    "            with open(path, \"rb\") as fp:   # Unpickling\n",
    "                self.buffer = pickle.load(fp)\n",
    "        \n",
    "            print(\"Replay Buffer loaded : \" + str(len(self.buffer)))\n",
    "        except EOFError:\n",
    "            \n",
    "            print(\"Replay Buffer Load file empty\")\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e25a8337",
   "metadata": {},
   "source": [
    "## Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fb87db71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(action_values, tau=1.0):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        action_values (Numpy array): A 2D array of shape (batch_size, num_actions). \n",
    "                       The action-values computed by an action-value network.              \n",
    "        tau (float): The temperature parameter scalar.\n",
    "    Returns:\n",
    "        A 2D array of shape (batch_size, num_actions). Where each column is a probability distribution over\n",
    "        the actions representing the policy.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Compute the preferences by dividing the action-values by the temperature parameter tau\n",
    "    preferences = action_values/tau\n",
    "    # Compute the maximum preference across the actions\n",
    "    max_preference = np.max(preferences, axis=1)\n",
    "        \n",
    "    \n",
    "    # Reshape max_preference array which has shape [Batch,] to [Batch, 1]. This allows NumPy broadcasting \n",
    "    # when subtracting the maximum preference from the preference of each action.\n",
    "    reshaped_max_preference = max_preference.reshape((-1, 1))\n",
    "    \n",
    "    # Compute the numerator, i.e., the exponential of the preference - the max preference.\n",
    "    exp_preferences = np.exp(preferences - reshaped_max_preference)\n",
    "    # Compute the denominator, i.e., the sum over the numerator along the actions axis.\n",
    "    sum_of_exp_preferences = np.sum(exp_preferences, axis=1)\n",
    "    \n",
    "   \n",
    "    \n",
    "    # Reshape sum_of_exp_preferences array which has shape [Batch,] to [Batch, 1] to  allow for NumPy broadcasting \n",
    "    # when dividing the numerator by the denominator.\n",
    "    reshaped_sum_of_exp_preferences = sum_of_exp_preferences.reshape((-1, 1))\n",
    "    \n",
    "    # Compute the action probabilities according to the equation in the previous cell.\n",
    "    action_probs = exp_preferences/reshaped_sum_of_exp_preferences\n",
    "    \n",
    "   \n",
    "    \n",
    "    \n",
    "    # squeeze() removes any singleton dimensions. It is used here because this function is used in the \n",
    "    # agent policy when selecting an action (for which the batch dimension is 1.) As np.random.choice is used in \n",
    "    # the agent policy and it expects 1D arrays, we need to remove this singleton batch dimension.\n",
    "    action_probs = action_probs.squeeze()\n",
    "    return action_probs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0635bd91",
   "metadata": {},
   "source": [
    "## Compiling\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a50cccb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_td_error(states, next_states, actions, rewards, discount, terminals, network, current_q, tau):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        states (Numpy array): The batch of states with the shape (batch_size, state_dim).\n",
    "        next_states (Numpy array): The batch of next states with the shape (batch_size, state_dim).\n",
    "        actions (Numpy array): The batch of actions with the shape (batch_size,).\n",
    "        rewards (Numpy array): The batch of rewards with the shape (batch_size,).\n",
    "        discount (float): The discount factor.\n",
    "        terminals (Numpy array): The batch of terminals with the shape (batch_size,).\n",
    "        network (ActionValueNetwork): The latest state of the network that is getting replay updates.\n",
    "        current_q (ActionValueNetwork): The fixed network used for computing the targets, \n",
    "                                        and particularly, the action-values at the next-states.\n",
    "    Returns:\n",
    "        The TD errors (Numpy array) for actions taken, of shape (batch_size,)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Here network is the latest state of the network that is getting replay updates. In other words, \n",
    "    # the network represents Q_{t+1}^{i} whereas current_q represents Q_t, the fixed network used for computing the \n",
    "    # targets, and particularly, the action-values at the next-states.\n",
    "    \n",
    "    # Compute action values at next states using current_q network\n",
    "    # q_next_mat is a 2D array of shape (batch_size, num_actions)\n",
    "    \n",
    "    # Q(t+1)\n",
    "    q_next_mat = current_q.get_action_values(next_states)\n",
    " \n",
    "    # Compute policy at next state by passing the action-values in q_next_mat to softmax()\n",
    "    # probs_mat is a 2D array of shape (batch_size, num_actions)\n",
    "    \n",
    "    # Pi\n",
    "    probs_mat = softmax(q_next_mat, tau)\n",
    "\n",
    "    # Compute the estimate of the next state value, v_next_vec.\n",
    "    # v_next_vec is a 1D array of shape (batch_size,)\n",
    "\n",
    "    v_next_vec = np.sum(probs_mat * q_next_mat, axis=1) * (1-terminals)\n",
    "    \n",
    "    # Compute Expected Sarsa target\n",
    "    # target_vec is a 1D array of shape (batch_size,)\n",
    "    \n",
    "    # \n",
    "    target_vec = rewards + discount*v_next_vec\n",
    "\n",
    "    # Compute action values at the current states for all actions using network\n",
    "    # q_mat is a 2D array of shape (batch_size, num_actions)\n",
    "   \n",
    "    q_mat = network.get_action_values(states)\n",
    "\n",
    "    # Batch Indices is an array from 0 to the batch size - 1. \n",
    "    batch_indices = np.arange(q_mat.shape[0])\n",
    "\n",
    "    # Compute q_vec by selecting q(s, a) from q_mat for taken actions\n",
    "    # Use batch_indices as the index for the first dimension of q_mat\n",
    "    # q_vec is a 1D array of shape (batch_size)\n",
    "    \n",
    "#     q_vec = q_mat[batch_indices, int(actions)]\n",
    "\n",
    "    # Compute TD errors for actions taken\n",
    "    # delta_vec is a 1D array of shape (batch_size)\n",
    "    \n",
    "#     delta_vec = target_vec - q_vec\n",
    "    target = np.zeros(q_next_mat.shape)\n",
    "    for i in range(q_next_mat.shape[0]):\n",
    "        \n",
    "        target[i,int(actions[i])] = target_vec[i]\n",
    "        \n",
    "    return target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "43dcf7b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_network(experiences, discount, optimizer, network, current_q, tau):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        experiences (Numpy array): The batch of experiences including the states, actions, \n",
    "                                   rewards, terminals, and next_states.\n",
    "        discount (float): The discount factor.\n",
    "        network (ActionValueNetwork): The latest state of the network that is getting replay updates.\n",
    "        current_q (ActionValueNetwork): The fixed network used for computing the targets, \n",
    "                                        and particularly, the action-values at the next-states.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get states, action, rewards, terminals, and next_states from experiences\n",
    "    states, actions, rewards, terminals, next_states = map(list, zip(*experiences))\n",
    "    states = np.concatenate(states)\n",
    "    next_states = np.concatenate(next_states)\n",
    "    rewards = np.array(rewards)\n",
    "    terminals = np.array(terminals)\n",
    "    batch_size = states.shape[0]\n",
    "    loss_function = keras.losses.Huber()\n",
    "    # Compute TD error using the get_td_error function\n",
    "    # q_vec is a 1D array of shape (batch_size)\n",
    "    target_vec = get_td_error(states, next_states, actions, rewards, discount, terminals, network, current_q, tau)\n",
    "#     tensorboard = TensorBoard(log_dir=\"logs/{}\".format(NAME))\n",
    "    network.model.fit(states, target_vec, epochs=1, verbose=0)\n",
    "#     callbacks=[tensorboard]\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "003c90e3",
   "metadata": {},
   "source": [
    "## Agent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "60c28d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent(BaseAgent):\n",
    "    def __init__(self):\n",
    "        self.name = \"expected_sarsa_agent\"\n",
    "        self.network = None\n",
    "    def agent_init(self, agent_config):\n",
    "        \"\"\"Setup for the agent called when the experiment first starts.\n",
    "\n",
    "        Set parameters needed to setup the agent.\n",
    "\n",
    "        Assume agent_config dict contains:\n",
    "        {\n",
    "            network_config: dictionary,\n",
    "            optimizer_config: dictionary,\n",
    "            replay_buffer_size: integer,\n",
    "            minibatch_sz: integer, \n",
    "            num_replay_updates_per_step: float\n",
    "            discount_factor: float,\n",
    "        }\n",
    "        \"\"\"\n",
    "        self.replay_buffer = ReplayBuffer(agent_config['replay_buffer_size'], \n",
    "                                          agent_config['minibatch_sz'], agent_config.get(\"seed\"), agent_config[\"dirc\"], agent_config[\"loadBool\"])\n",
    "        self.network = ActionValueNetwork(agent_config['network_config'])\n",
    "        self.network_target = ActionValueNetwork(agent_config['network_config'])\n",
    "        self.num_actions = agent_config['network_config']['num_actions']\n",
    "        self.num_replay = agent_config['num_replay_updates_per_step']\n",
    "        self.discount = agent_config['gamma']\n",
    "        self.tau = agent_config['tau']\n",
    "        \n",
    "        self.rand_generator = np.random.RandomState(agent_config.get(\"seed\"))\n",
    "        self.epsilon = 0.01\n",
    "        self.last_state = None\n",
    "        self.last_action = None\n",
    "        self.sum_rewards = 0\n",
    "        self.episode_steps = 0\n",
    "        self.optimizer = keras.optimizers.Adam(learning_rate=0.00025, clipnorm=1.0)\n",
    "        self.action_space = np.array([i for i in range(self.num_actions)])\n",
    "        self.action_space = self.action_space.astype('float32')\n",
    "        self.eps = 1\n",
    "        self.eps_min = 0.1\n",
    "        self.eps_decay = 0.999\n",
    "\n",
    "    def policy(self, state):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            state (Numpy array): the state.\n",
    "        Returns:\n",
    "            the action. \n",
    "        \"\"\"\n",
    "        if np.random.rand() > self.epsilon:\n",
    "            \n",
    "            action_values = self.network.get_action_values(state)\n",
    "            probs_batch = softmax(action_values, self.tau)\n",
    "            action = self.rand_generator.choice(self.num_actions, p=probs_batch.squeeze())\n",
    "        else:\n",
    "            action = np.random.choice(self.action_space)\n",
    "        return action\n",
    "\n",
    "    def agent_start(self, state):\n",
    "        \"\"\"The first method called when the experiment starts, called after\n",
    "        the environment starts.\n",
    "        Args:\n",
    "            state (Numpy array): the state from the\n",
    "                environment's evn_start function.\n",
    "        Returns:\n",
    "            The first action the agent takes.\n",
    "        \"\"\"\n",
    "        self.sum_rewards = 0\n",
    "        self.episode_steps = 0\n",
    "        self.last_state = np.array([state])\n",
    "        self.last_action = self.policy(self.last_state)\n",
    "        return self.last_action\n",
    "\n",
    "\n",
    "    def agent_step(self, reward, state):\n",
    "        \"\"\"A step taken by the agent.\n",
    "        Args:\n",
    "            reward (float): the reward received for taking the last action taken\n",
    "            state (Numpy array): the state from the\n",
    "                environment's step based, where the agent ended up after the\n",
    "                last step\n",
    "        Returns:\n",
    "            The action the agent is taking.\n",
    "        \"\"\"\n",
    "        \n",
    "        self.sum_rewards += reward\n",
    "        self.episode_steps += 1\n",
    "        \n",
    "        # Make state an array of shape (1, state_dim) to add a batch dimension and\n",
    "        # to later match the get_action_values() and get_TD_update() functions\n",
    "        state = np.array([state])\n",
    "\n",
    "        # Select action\n",
    "        action = self.policy(state)\n",
    "        \n",
    "        # Append new experience to replay buffer\n",
    "        self.replay_buffer.append(self.last_state, self.last_action, reward, 0, state)\n",
    "        \n",
    "        # Perform replay steps:\n",
    "        if self.replay_buffer.size() > self.replay_buffer.minibatch_size:\n",
    "#             current_q = deepcopy(self.network)\n",
    "            self.network_target.set_weights(self.network.get_weights())\n",
    "            for _ in range(self.num_replay):\n",
    "                \n",
    "                # Get sample experiences from the replay buffer\n",
    "                experiences = self.replay_buffer.sample()\n",
    "                \n",
    "                # Call optimize_network to update the weights of the network \n",
    "                optimize_network(experiences, self.discount, self.optimizer, self.network, self.network_target, self.tau)\n",
    "                \n",
    "        # Update the last state and last action.\n",
    "        self.last_state = state\n",
    "        self.last_action = action\n",
    "        \n",
    "        if self.eps > self.eps_min:\n",
    "            self.eps *= self.eps_decay\n",
    "        \n",
    "        \n",
    "        return action\n",
    "\n",
    "    def agent_end(self, reward):\n",
    "        \"\"\"Run when the agent terminates.\n",
    "        Args:\n",
    "            reward (float): the reward the agent received for entering the\n",
    "                terminal state.\n",
    "        \"\"\"\n",
    "        self.sum_rewards += reward\n",
    "        self.episode_steps += 1\n",
    "        \n",
    "        # Set terminal state to an array of zeros\n",
    "        state = np.zeros_like(self.last_state)\n",
    "\n",
    "        # Append new experience to replay buffer       \n",
    "       \n",
    "        self.replay_buffer.append(self.last_state, self.last_action, reward, 1, state)\n",
    "        \n",
    "        # Perform replay steps:\n",
    "        if self.replay_buffer.size() > self.replay_buffer.minibatch_size:\n",
    "#             current_q = deepcopy(self.network)\n",
    "            self.network_target.set_weights(self.network.get_weights())\n",
    "            for _ in range(self.num_replay):\n",
    "                \n",
    "                # Get sample experiences from the replay buffer\n",
    "                experiences = self.replay_buffer.sample()\n",
    "                \n",
    "                # Call optimize_network to update the weights of the network\n",
    "                optimize_network(experiences, self.discount, self.optimizer, self.network, self.network_target, self.tau)\n",
    "                \n",
    "        \n",
    "    def trained_agent_step(self, reward, state):\n",
    "        \"\"\"A step taken by the trained agent.\n",
    "        Args:\n",
    "            reward (float): the reward received for taking the last action taken\n",
    "            state (Numpy array): the state from the\n",
    "                environment's step based, where the agent ended up after the\n",
    "                last step\n",
    "        Returns:\n",
    "            The action the agent is taking.\n",
    "        \"\"\"\n",
    "        \n",
    "        \n",
    "        self.sum_rewards += reward\n",
    "        # Make state an array of shape (1, state_dim) to add a batch dimension and\n",
    "        # to later match the get_action_values() and get_TD_update() functions\n",
    "        state = np.array([state])\n",
    "\n",
    "        # Select action\n",
    "        action = self.policy(state)\n",
    "        \n",
    "        return action\n",
    "    \n",
    "    def agent_message(self, message):\n",
    "        if message == \"get_sum_reward\":\n",
    "            return self.sum_rewards\n",
    "        else:\n",
    "            raise Exception(\"Unrecognized Message!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fbbbe316",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_weights_to_json(model, file_name, path):\n",
    "    model_json = model.to_json()\n",
    "    file_name1 = ( str(file_name)+\".json\")\n",
    "#     path = os.path.join(path, file_name1)\n",
    "    pathx = path + file_name1\n",
    "    with open(pathx , \"w\") as json_file:\n",
    "        json_file.write(model_json)\n",
    "    # serialize weights to HDF5\n",
    "    model_json = model.to_json()\n",
    "    file_name2 = ( str(file_name)+\".h5\")\n",
    "#     path = os.path.join(path, file_name2)\n",
    "    \n",
    "    pathx = path + file_name2\n",
    "    model.save_weights(pathx)\n",
    "    print(\"Saved model to disk\")\n",
    "    \n",
    "def loaf_weights_from_json(model, file_name, path):\n",
    "    file_name1 = ( str(file_name)+\".json\")\n",
    "    path = os.path.join(path, file_name1)\n",
    "    json_file = open(path, 'r')\n",
    "    loaded_model_json = json_file.read()\n",
    "    json_file.close()\n",
    "    loaded_model = model_from_json(loaded_model_json)\n",
    "    # load weights into new model\n",
    "    file_name2 = ( str(file_name)+\".h5\")\n",
    "    path = os.path.join(path, file_name2)\n",
    "    loaded_model.load_weights(path)\n",
    "    print(\"Loaded model from disk\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21f250b0",
   "metadata": {},
   "source": [
    "## Run the experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abe58618",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                            | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Replay Buffer loaded : 625\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def run_experiment(environment, agent, environment_parameters, agent_parameters, experiment_parameters):\n",
    "    \n",
    "    rl_glue = RLGlue(environment, agent)\n",
    "        \n",
    "    # save sum of reward at the end of each episode\n",
    "    agent_sum_reward = np.zeros((experiment_parameters[\"num_runs\"], \n",
    "                                 experiment_parameters[\"num_episodes\"]))\n",
    "    trained_agent_sum_reward = np.zeros((experiment_parameters[\"num_runs\"], \n",
    "                                 experiment_parameters[\"num_episodes_trained\"]))\n",
    "\n",
    "    env_info = {}\n",
    "\n",
    "    agent_info = agent_parameters\n",
    "    \n",
    "    # one agent setting\n",
    "    for run in range(1, experiment_parameters[\"num_runs\"]+1):\n",
    "        agent_info[\"seed\"] = run\n",
    "        agent_info[\"network_config\"][\"seed\"] = run\n",
    "        env_info[\"seed\"] = run\n",
    "\n",
    "        rl_glue.rl_init(agent_info, env_info)\n",
    "        \n",
    "            \n",
    "        for episode in tqdm(range(1, experiment_parameters[\"num_episodes\"]+1)):\n",
    "            # run episode\n",
    "            rl_glue.rl_episode(experiment_parameters[\"timeout\"])\n",
    "            \n",
    "            episode_reward = rl_glue.rl_agent_message(\"get_sum_reward\")\n",
    "            agent_sum_reward[run - 1, episode - 1] = episode_reward\n",
    "            \n",
    "        \n",
    "        for episode in range(1, experiment_parameters[\"num_episodes_trained\"]+1):\n",
    "            rl_glue.trained_rl_episode(experiment_parameters[\"timeout\"])\n",
    "            trained_agent_sum_reward[run - 1, episode - 1 ] =  rl_glue.rl_env_message(\"get_sum_reward\")\n",
    "            \n",
    "            print(\"Trained reward = \" + str(trained_agent_sum_reward))\n",
    "    save_name = \"{}\".format(rl_glue.agent.name)\n",
    "    if not os.path.exists('results'):\n",
    "        os.makedirs('results')\n",
    "    np.save(\"results/sum_reward_{}\".format(save_name), agent_sum_reward)\n",
    "    shutil.make_archive('results', 'zip', 'results')\n",
    "    save_weights_to_json(rl_glue.agent.network.model, NAME_Save, \"weights/\")\n",
    "    rl_glue.agent.replay_buffer.save()\n",
    "    \n",
    "# Run Experiment\n",
    "# tf.debugging.set_log_device_placement(True)\n",
    "# Experiment parameters\n",
    "experiment_parameters = {\n",
    "    \"num_runs\" : 1,\n",
    "    \"num_episodes_trained\" : 0,\n",
    "    \"num_episodes\" : 1,\n",
    "    # OpenAI Gym environments allow for a timestep limit timeout, causing episodes to end after \n",
    "    # some number of timesteps. Here we use the default of 500.\n",
    "    \"timeout\" : 500\n",
    "}\n",
    "\n",
    "# Environment parameters\n",
    "environment_parameters = {}\n",
    "\n",
    "current_env = AtariBreakoutEnvironment\n",
    "\n",
    "# Agent parameters\n",
    "agent_parameters = {\n",
    "    'network_config': {\n",
    "        'state_dim': [210,160,3],\n",
    "        'num_actions': 4,\n",
    "        'learning_rate':0.001\n",
    "    },\n",
    "    'replay_buffer_size': 5000,\n",
    "    'minibatch_sz': 4,\n",
    "    'dirc' : \"ReplayBuffer\",\n",
    "    'loadBool' : True,\n",
    "    'num_replay_updates_per_step': 4,\n",
    "    'gamma': 0.99,\n",
    "    'tau': 0.001\n",
    "}\n",
    "current_agent = Agent\n",
    "\n",
    "# run experiment\n",
    "run_experiment(current_env, current_agent, environment_parameters, agent_parameters, experiment_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "604a3286",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_result([\"expected_sarsa_agent\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "57d98a71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2.]]\n"
     ]
    }
   ],
   "source": [
    "data = np.load('results/sum_reward_expected_sarsa_agent.npy') \n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7683765f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
