{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c881f145",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from rl_glue import RLGlue\n",
    "\n",
    "from environment import BaseEnvironment\n",
    "from atari_breakout import AtariBreakoutEnvironment\n",
    "from agent import BaseAgent \n",
    "\n",
    "from collections import deque\n",
    "from copy import deepcopy\n",
    "from tqdm import tqdm\n",
    "import os \n",
    "import time\n",
    "import shutil\n",
    "from plot_script import plot_result\n",
    "import pickle\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential, model_from_json\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a5ddb28",
   "metadata": {},
   "source": [
    "## Neural Network for action values\n",
    "\n",
    "We use a neural network with one hidden layer for approximating the action-value function in a control problem. The output layer size is the number of actions. \n",
    "\n",
    "The get_action_values() function computes the action-value function by doing a forward pass.\n",
    "The get_TD_update() function computes the gradient of the action-value function with respect to the weights times the TD error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "42e83475",
   "metadata": {},
   "outputs": [],
   "source": [
    "NAME_Save = \"Breakout-CNN-C-32-8-4__Mpad__C-64-4-3__Mpad__C-64-3-1__Mpad__D-512-{}\".format(int(time.time()))\n",
    "NAME_Load = \"\"\n",
    "class ActionValueNetwork:\n",
    "\n",
    "    def __init__(self, network_config):\n",
    "        self.state_dim = network_config.get(\"state_dim\")\n",
    "        self.num_actions = network_config.get(\"num_actions\")\n",
    "        self.learning_rate = network_config.get(\"learning_rate\")\n",
    "        \n",
    "#         inputs = layers.Input(shape=(self.state_dim, self.state_dim, 3))\n",
    "#         # Convolutions on the frames on the screen\n",
    "#         layer1 = layers.Conv2D(6, 7, strides=4, activation=\"relu\")(inputs)\n",
    "#         layer2 = layers.MaxPooling2D(pool_size = (2,2))(layer1)\n",
    "#         layer3 = layers.Conv2D(12, 4, strides=2, activation=\"relu\")(layer2)\n",
    "#         layer4 = layers.MaxPooling2D(pool_size = (2,2))(layer3)\n",
    "#         layer5 = layers.Flatten()(layer4)\n",
    "#         layer6 = layers.Dense(512, activation=\"relu\")(layer5)\n",
    "#         action = layers.Dense(self.num_actions, activation=\"linear\")(layer6)\n",
    "\n",
    "#         self.model =  keras.Model(inputs=inputs, outputs=action)\n",
    "        \n",
    "        model = Sequential()\n",
    "#         model.add(layers.Input(shape=(self.state_dim, self.state_dim, 3)))\n",
    "        model.add(layers.Conv2D(filters = 32, kernel_size = 8, strides = 4, activation=\"relu\", input_shape = (self.state_dim[0], self.state_dim[1], self.state_dim[2])))\n",
    "        \n",
    "        model.add(layers.MaxPooling2D(pool_size = (2,2), padding = 'same'))\n",
    "        model.add(layers.Conv2D(filters = 64, kernel_size = 4, strides = 3, activation=\"relu\"))\n",
    "        model.add(layers.MaxPooling2D(pool_size = (2,2), padding = 'same'))\n",
    "        model.add(layers.Conv2D(filters = 64, kernel_size = 3, strides = 1, activation=\"relu\"))\n",
    "        model.add(layers.MaxPooling2D(pool_size = (2,2), padding = 'same'))\n",
    "        model.add(layers.Flatten())\n",
    "        model.add(layers.Dense(512, activation=\"relu\"))\n",
    "        model.add(layers.Dense(self.num_actions, activation=None))\n",
    "              \n",
    "        model.compile(loss = 'mean_squared_error', optimizer=Adam(lr=self.learning_rate, epsilon=1e-7))\n",
    "        self.model = model\n",
    "#         model.summary()\n",
    "#         model = models.Sequential()\n",
    "#         model.add(layers.Conv2D(32, 8, strides = 4, activation='relu', input_shape=(32, 32, 3)))\n",
    "#         model.add(layers.MaxPooling2D((2, 2)))\n",
    "#         model.add(layers.Conv2D(64, 4, strides = 2, activation='relu'))\n",
    "#         model.add(layers.MaxPooling2D((2, 2)))\n",
    "#         model.add(layers.Conv2D(64, 3, activation='relu'))\n",
    "#         model.add(layers.Flatten())\n",
    "#         model.add(layers.Dense(512, activation=\"relu\")(layer4))\n",
    "#         model.add(layers.Dense(self.num_actions, activation=\"linear\"))\n",
    "        \n",
    "#         self.model = model\n",
    "\n",
    "    def get_action_values(self, s):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            s (Numpy array): The state.\n",
    "        Returns:\n",
    "            The action-values (Numpy array) calculated using the network's weights.\n",
    "        \"\"\"\n",
    "        \n",
    "        q_vals = self.model.predict(s)\n",
    "\n",
    "        return q_vals\n",
    "    \n",
    "\n",
    "#     def get_TD_update(self, s, delta_mat):\n",
    "#         \"\"\"\n",
    "#         Args:\n",
    "#             s (Numpy array): The state.\n",
    "#             delta_mat (Numpy array): A 2D array of shape (batch_size, num_actions). Each row of delta_mat  \n",
    "#             correspond to one state in the batch. Each row has only one non-zero element \n",
    "#             which is the TD-error corresponding to the action taken.\n",
    "#         Returns:\n",
    "#             The TD update (Array of dictionaries with gradient times TD errors) for the network's weights\n",
    "#         \"\"\"\n",
    "\n",
    "#         W0, b0 = self.weights[0]['W'], self.weights[0]['b']\n",
    "#         W1, b1 = self.weights[1]['W'], self.weights[1]['b']\n",
    "        \n",
    "#         psi = np.dot(s, W0) + b0\n",
    "#         x = np.maximum(psi, 0)\n",
    "#         dx = (psi > 0).astype(float)\n",
    "\n",
    "#         # td_update has the same structure as self.weights, that is an array of dictionaries.\n",
    "#         # td_update[0][\"W\"], td_update[0][\"b\"], td_update[1][\"W\"], and td_update[1][\"b\"] have the same shape as \n",
    "#         # self.weights[0][\"W\"], self.weights[0][\"b\"], self.weights[1][\"W\"], and self.weights[1][\"b\"] respectively\n",
    "#         td_update = [dict() for i in range(len(self.weights))]\n",
    "         \n",
    "#         v = delta_mat\n",
    "#         td_update[1]['W'] = np.dot(x.T, v) * 1. / s.shape[0]\n",
    "#         td_update[1]['b'] = np.sum(v, axis=0, keepdims=True) * 1. / s.shape[0]\n",
    "        \n",
    "#         v = np.dot(v, W1.T) * dx\n",
    "#         td_update[0]['W'] = np.dot(s.T, v) * 1. / s.shape[0]\n",
    "#         td_update[0]['b'] = np.sum(v, axis=0, keepdims=True) * 1. / s.shape[0]\n",
    "                \n",
    "#         return td_update\n",
    "        \n",
    "    \n",
    "    def get_weights(self):\n",
    "        return self.model.get_weights()\n",
    "    \n",
    "    def set_weights(self, weights):\n",
    "        self.model.set_weights(weights)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db59e3e5",
   "metadata": {},
   "source": [
    "## Replay Buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1447670b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, size, minibatch_size, seed, dirc, loadBool):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            size (integer): The size of the replay buffer.              \n",
    "            minibatch_size (integer): The sample size.\n",
    "            seed (integer): The seed for the random number generator. \n",
    "        \"\"\"\n",
    "        self.buffer = []\n",
    "        self.minibatch_size = minibatch_size\n",
    "        self.rand_generator = np.random.RandomState(seed)\n",
    "        self.max_size = size\n",
    "        self.directory = dirc\n",
    "        if loadBool:\n",
    "            self.buffer = self.load()\n",
    "    def append(self, state, action, reward, terminal, next_state):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            state (Numpy array): The state.              \n",
    "            action (integer): The action.\n",
    "            reward (float): The reward.\n",
    "            terminal (integer): 1 if the next state is a terminal state and 0 otherwise.\n",
    "            next_state (Numpy array): The next state.           \n",
    "        \"\"\"\n",
    "        if len(self.buffer) == self.max_size:\n",
    "            del self.buffer[0]\n",
    "        state = state.astype('float32')\n",
    "        next_state = next_state.astype('float32')\n",
    "        self.buffer.append([state, action, reward, terminal, next_state])\n",
    "\n",
    "    def sample(self):\n",
    "        \"\"\"\n",
    "        Returns:\n",
    "            A list of transition tuples including state, action, reward, terinal, and next_state\n",
    "        \"\"\"\n",
    "        idxs = self.rand_generator.choice(np.arange(len(self.buffer)), size=self.minibatch_size)\n",
    "        return [self.buffer[idx] for idx in idxs]\n",
    "\n",
    "    def size(self):\n",
    "        return len(self.buffer)\n",
    "    \n",
    "    def save(self):\n",
    "        path = os.path.join(self.directory, \"replay_buffer_1.txt\")\n",
    "        with open(path, \"wb\") as fp:   #Pickling\n",
    "            pickle.dump(self.buffer, fp)\n",
    "        print(\"Replay Buffer saved : \" + str(len(self.buffer)) )\n",
    "        \n",
    "    def load(self):\n",
    "        try:\n",
    "            path = os.path.join(self.directory, \"replay_buffer_1.txt\")\n",
    "            with open(path, \"rb\") as fp:   # Unpickling\n",
    "                self.buffer = pickle.load(fp)\n",
    "        \n",
    "            print(\"Replay Buffer loaded : \" + str(len(self.buffer)))\n",
    "        except EOFError:\n",
    "            \n",
    "            print(\"Replay Buffer Load file empty\")\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e25a8337",
   "metadata": {},
   "source": [
    "## Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fb87db71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(action_values, tau=1.0):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        action_values (Numpy array): A 2D array of shape (batch_size, num_actions). \n",
    "                       The action-values computed by an action-value network.              \n",
    "        tau (float): The temperature parameter scalar.\n",
    "    Returns:\n",
    "        A 2D array of shape (batch_size, num_actions). Where each column is a probability distribution over\n",
    "        the actions representing the policy.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Compute the preferences by dividing the action-values by the temperature parameter tau\n",
    "    preferences = action_values/tau\n",
    "    # Compute the maximum preference across the actions\n",
    "    max_preference = np.max(preferences, axis=1)\n",
    "        \n",
    "    \n",
    "    # Reshape max_preference array which has shape [Batch,] to [Batch, 1]. This allows NumPy broadcasting \n",
    "    # when subtracting the maximum preference from the preference of each action.\n",
    "    reshaped_max_preference = max_preference.reshape((-1, 1))\n",
    "    \n",
    "    # Compute the numerator, i.e., the exponential of the preference - the max preference.\n",
    "    exp_preferences = np.exp(preferences - reshaped_max_preference)\n",
    "    # Compute the denominator, i.e., the sum over the numerator along the actions axis.\n",
    "    sum_of_exp_preferences = np.sum(exp_preferences, axis=1)\n",
    "    \n",
    "   \n",
    "    \n",
    "    # Reshape sum_of_exp_preferences array which has shape [Batch,] to [Batch, 1] to  allow for NumPy broadcasting \n",
    "    # when dividing the numerator by the denominator.\n",
    "    reshaped_sum_of_exp_preferences = sum_of_exp_preferences.reshape((-1, 1))\n",
    "    \n",
    "    # Compute the action probabilities according to the equation in the previous cell.\n",
    "    action_probs = exp_preferences/reshaped_sum_of_exp_preferences\n",
    "    \n",
    "   \n",
    "    \n",
    "    \n",
    "    # squeeze() removes any singleton dimensions. It is used here because this function is used in the \n",
    "    # agent policy when selecting an action (for which the batch dimension is 1.) As np.random.choice is used in \n",
    "    # the agent policy and it expects 1D arrays, we need to remove this singleton batch dimension.\n",
    "    action_probs = action_probs.squeeze()\n",
    "    return action_probs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0635bd91",
   "metadata": {},
   "source": [
    "## Compiling\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a50cccb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_td_error(states, next_states, actions, rewards, discount, terminals, network, current_q, tau):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        states (Numpy array): The batch of states with the shape (batch_size, state_dim).\n",
    "        next_states (Numpy array): The batch of next states with the shape (batch_size, state_dim).\n",
    "        actions (Numpy array): The batch of actions with the shape (batch_size,).\n",
    "        rewards (Numpy array): The batch of rewards with the shape (batch_size,).\n",
    "        discount (float): The discount factor.\n",
    "        terminals (Numpy array): The batch of terminals with the shape (batch_size,).\n",
    "        network (ActionValueNetwork): The latest state of the network that is getting replay updates.\n",
    "        current_q (ActionValueNetwork): The fixed network used for computing the targets, \n",
    "                                        and particularly, the action-values at the next-states.\n",
    "    Returns:\n",
    "        The TD errors (Numpy array) for actions taken, of shape (batch_size,)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Here network is the latest state of the network that is getting replay updates. In other words, \n",
    "    # the network represents Q_{t+1}^{i} whereas current_q represents Q_t, the fixed network used for computing the \n",
    "    # targets, and particularly, the action-values at the next-states.\n",
    "    \n",
    "    # Compute action values at next states using current_q network\n",
    "    # q_next_mat is a 2D array of shape (batch_size, num_actions)\n",
    "    \n",
    "    # Q(t+1)\n",
    "    q_next_mat = current_q.get_action_values(next_states)\n",
    " \n",
    "    # Compute policy at next state by passing the action-values in q_next_mat to softmax()\n",
    "    # probs_mat is a 2D array of shape (batch_size, num_actions)\n",
    "    \n",
    "    # Pi\n",
    "    probs_mat = softmax(q_next_mat, tau)\n",
    "\n",
    "    # Compute the estimate of the next state value, v_next_vec.\n",
    "    # v_next_vec is a 1D array of shape (batch_size,)\n",
    "\n",
    "    v_next_vec = np.sum(probs_mat * q_next_mat, axis=1) * (1-terminals)\n",
    "    \n",
    "    # Compute Expected Sarsa target\n",
    "    # target_vec is a 1D array of shape (batch_size,)\n",
    "    \n",
    "    # \n",
    "    target_vec = rewards + discount*v_next_vec\n",
    "\n",
    "    # Compute action values at the current states for all actions using network\n",
    "    # q_mat is a 2D array of shape (batch_size, num_actions)\n",
    "   \n",
    "    q_mat = network.get_action_values(states)\n",
    "\n",
    "    # Batch Indices is an array from 0 to the batch size - 1. \n",
    "    batch_indices = np.arange(q_mat.shape[0])\n",
    "\n",
    "    # Compute q_vec by selecting q(s, a) from q_mat for taken actions\n",
    "    # Use batch_indices as the index for the first dimension of q_mat\n",
    "    # q_vec is a 1D array of shape (batch_size)\n",
    "    \n",
    "#     q_vec = q_mat[batch_indices, int(actions)]\n",
    "\n",
    "    # Compute TD errors for actions taken\n",
    "    # delta_vec is a 1D array of shape (batch_size)\n",
    "    \n",
    "#     delta_vec = target_vec - q_vec\n",
    "    target = np.zeros(q_next_mat.shape)\n",
    "    for i in range(q_next_mat.shape[0]):\n",
    "        \n",
    "        target[i,int(actions[i])] = target_vec[i]\n",
    "        \n",
    "    return target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "43dcf7b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_network(experiences, discount, optimizer, network, current_q, tau):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        experiences (Numpy array): The batch of experiences including the states, actions, \n",
    "                                   rewards, terminals, and next_states.\n",
    "        discount (float): The discount factor.\n",
    "        network (ActionValueNetwork): The latest state of the network that is getting replay updates.\n",
    "        current_q (ActionValueNetwork): The fixed network used for computing the targets, \n",
    "                                        and particularly, the action-values at the next-states.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get states, action, rewards, terminals, and next_states from experiences\n",
    "    states, actions, rewards, terminals, next_states = map(list, zip(*experiences))\n",
    "    states = np.concatenate(states)\n",
    "    next_states = np.concatenate(next_states)\n",
    "    rewards = np.array(rewards)\n",
    "    terminals = np.array(terminals)\n",
    "    batch_size = states.shape[0]\n",
    "    loss_function = keras.losses.Huber()\n",
    "    # Compute TD error using the get_td_error function\n",
    "    # q_vec is a 1D array of shape (batch_size)\n",
    "    target_vec = get_td_error(states, next_states, actions, rewards, discount, terminals, network, current_q, tau)\n",
    "#     tensorboard = TensorBoard(log_dir=\"logs/{}\".format(NAME))\n",
    "    network.model.fit(states, target_vec, epochs=1, verbose=0)\n",
    "#     callbacks=[tensorboard]\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "003c90e3",
   "metadata": {},
   "source": [
    "## Agent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "60c28d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent(BaseAgent):\n",
    "    def __init__(self):\n",
    "        self.name = \"expected_sarsa_agent\"\n",
    "        self.network = None\n",
    "    def agent_init(self, agent_config):\n",
    "        \"\"\"Setup for the agent called when the experiment first starts.\n",
    "\n",
    "        Set parameters needed to setup the agent.\n",
    "\n",
    "        Assume agent_config dict contains:\n",
    "        {\n",
    "            network_config: dictionary,\n",
    "            optimizer_config: dictionary,\n",
    "            replay_buffer_size: integer,\n",
    "            minibatch_sz: integer, \n",
    "            num_replay_updates_per_step: float\n",
    "            discount_factor: float,\n",
    "        }\n",
    "        \"\"\"\n",
    "        self.replay_buffer = ReplayBuffer(agent_config['replay_buffer_size'], \n",
    "                                          agent_config['minibatch_sz'], agent_config.get(\"seed\"), agent_config[\"dirc\"], agent_config[\"loadBool\"])\n",
    "        self.network = ActionValueNetwork(agent_config['network_config'])\n",
    "        self.network_target = ActionValueNetwork(agent_config['network_config'])\n",
    "        self.num_actions = agent_config['network_config']['num_actions']\n",
    "        self.num_replay = agent_config['num_replay_updates_per_step']\n",
    "        self.discount = agent_config['gamma']\n",
    "        self.tau = agent_config['tau']\n",
    "        \n",
    "        self.rand_generator = np.random.RandomState(agent_config.get(\"seed\"))\n",
    "        self.epsilon = 0.01\n",
    "        self.last_state = None\n",
    "        self.last_action = None\n",
    "        self.sum_rewards = 0\n",
    "        self.episode_steps = 0\n",
    "        self.optimizer = keras.optimizers.Adam(learning_rate=0.00025, clipnorm=1.0)\n",
    "        self.action_space = np.array([i for i in range(self.num_actions)])\n",
    "        self.action_space = self.action_space.astype('float32')\n",
    "        self.eps = 1\n",
    "        self.eps_min = 0.1\n",
    "        self.eps_decay = 0.999\n",
    "\n",
    "    def policy(self, state):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            state (Numpy array): the state.\n",
    "        Returns:\n",
    "            the action. \n",
    "        \"\"\"\n",
    "        if np.random.rand() > self.epsilon:\n",
    "            \n",
    "            action_values = self.network.get_action_values(state)\n",
    "            probs_batch = softmax(action_values, self.tau)\n",
    "            action = self.rand_generator.choice(self.num_actions, p=probs_batch.squeeze())\n",
    "        else:\n",
    "            action = np.random.choice(self.action_space)\n",
    "        return action\n",
    "\n",
    "    def agent_start(self, state):\n",
    "        \"\"\"The first method called when the experiment starts, called after\n",
    "        the environment starts.\n",
    "        Args:\n",
    "            state (Numpy array): the state from the\n",
    "                environment's evn_start function.\n",
    "        Returns:\n",
    "            The first action the agent takes.\n",
    "        \"\"\"\n",
    "        self.sum_rewards = 0\n",
    "        self.episode_steps = 0\n",
    "        self.last_state = np.array([state])\n",
    "        self.last_action = self.policy(self.last_state)\n",
    "        return self.last_action\n",
    "\n",
    "\n",
    "    def agent_step(self, reward, state):\n",
    "        \"\"\"A step taken by the agent.\n",
    "        Args:\n",
    "            reward (float): the reward received for taking the last action taken\n",
    "            state (Numpy array): the state from the\n",
    "                environment's step based, where the agent ended up after the\n",
    "                last step\n",
    "        Returns:\n",
    "            The action the agent is taking.\n",
    "        \"\"\"\n",
    "        \n",
    "        self.sum_rewards += reward\n",
    "        self.episode_steps += 1\n",
    "        \n",
    "        # Make state an array of shape (1, state_dim) to add a batch dimension and\n",
    "        # to later match the get_action_values() and get_TD_update() functions\n",
    "        state = np.array([state])\n",
    "\n",
    "        # Select action\n",
    "        action = self.policy(state)\n",
    "        \n",
    "        # Append new experience to replay buffer\n",
    "        self.replay_buffer.append(self.last_state, self.last_action, reward, 0, state)\n",
    "        \n",
    "        # Perform replay steps:\n",
    "        if self.replay_buffer.size() > self.replay_buffer.minibatch_size:\n",
    "#             current_q = deepcopy(self.network)\n",
    "            self.network_target.set_weights(self.network.get_weights())\n",
    "            for _ in range(self.num_replay):\n",
    "                \n",
    "                # Get sample experiences from the replay buffer\n",
    "                experiences = self.replay_buffer.sample()\n",
    "                \n",
    "                # Call optimize_network to update the weights of the network \n",
    "                optimize_network(experiences, self.discount, self.optimizer, self.network, self.network_target, self.tau)\n",
    "                \n",
    "        # Update the last state and last action.\n",
    "        self.last_state = state\n",
    "        self.last_action = action\n",
    "        \n",
    "        if self.eps > self.eps_min:\n",
    "            self.eps *= self.eps_decay\n",
    "        \n",
    "        \n",
    "        return action\n",
    "\n",
    "    def agent_end(self, reward):\n",
    "        \"\"\"Run when the agent terminates.\n",
    "        Args:\n",
    "            reward (float): the reward the agent received for entering the\n",
    "                terminal state.\n",
    "        \"\"\"\n",
    "        self.sum_rewards += reward\n",
    "        self.episode_steps += 1\n",
    "        \n",
    "        # Set terminal state to an array of zeros\n",
    "        state = np.zeros_like(self.last_state)\n",
    "\n",
    "        # Append new experience to replay buffer       \n",
    "       \n",
    "        self.replay_buffer.append(self.last_state, self.last_action, reward, 1, state)\n",
    "        \n",
    "        # Perform replay steps:\n",
    "        if self.replay_buffer.size() > self.replay_buffer.minibatch_size:\n",
    "#             current_q = deepcopy(self.network)\n",
    "            self.network_target.set_weights(self.network.get_weights())\n",
    "            for _ in range(self.num_replay):\n",
    "                \n",
    "                # Get sample experiences from the replay buffer\n",
    "                experiences = self.replay_buffer.sample()\n",
    "                \n",
    "                # Call optimize_network to update the weights of the network\n",
    "                optimize_network(experiences, self.discount, self.optimizer, self.network, self.network_target, self.tau)\n",
    "                \n",
    "        self.replay_buffer.save()\n",
    "    def trained_agent_step(self, reward, state):\n",
    "        \"\"\"A step taken by the trained agent.\n",
    "        Args:\n",
    "            reward (float): the reward received for taking the last action taken\n",
    "            state (Numpy array): the state from the\n",
    "                environment's step based, where the agent ended up after the\n",
    "                last step\n",
    "        Returns:\n",
    "            The action the agent is taking.\n",
    "        \"\"\"\n",
    "        \n",
    "        \n",
    "        self.sum_rewards += reward\n",
    "        # Make state an array of shape (1, state_dim) to add a batch dimension and\n",
    "        # to later match the get_action_values() and get_TD_update() functions\n",
    "        state = np.array([state])\n",
    "\n",
    "        # Select action\n",
    "        action = self.policy(state)\n",
    "        \n",
    "        return action\n",
    "    \n",
    "    def agent_message(self, message):\n",
    "        if message == \"get_sum_reward\":\n",
    "            return self.sum_rewards\n",
    "        else:\n",
    "            raise Exception(\"Unrecognized Message!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fbbbe316",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_weights_to_json(model, file_name, path):\n",
    "    model_json = model.to_json()\n",
    "    file_name1 = ( str(file_name)+\".json\")\n",
    "#     path = os.path.join(path, file_name1)\n",
    "    pathx = path + file_name1\n",
    "    with open(pathx , \"w\") as json_file:\n",
    "        json_file.write(model_json)\n",
    "    # serialize weights to HDF5\n",
    "    model_json = model.to_json()\n",
    "    file_name2 = ( str(file_name)+\".h5\")\n",
    "#     path = os.path.join(path, file_name2)\n",
    "    \n",
    "    pathx = path + file_name2\n",
    "    model.save_weights(pathx)\n",
    "    print(\"Saved model to disk\")\n",
    "    \n",
    "def loaf_weights_from_json(model, file_name, path):\n",
    "    file_name1 = ( str(file_name)+\".json\")\n",
    "    path = os.path.join(path, file_name1)\n",
    "    json_file = open(path, 'r')\n",
    "    loaded_model_json = json_file.read()\n",
    "    json_file.close()\n",
    "    loaded_model = model_from_json(loaded_model_json)\n",
    "    # load weights into new model\n",
    "    file_name2 = ( str(file_name)+\".h5\")\n",
    "    path = os.path.join(path, file_name2)\n",
    "    loaded_model.load_weights(path)\n",
    "    print(\"Loaded model from disk\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21f250b0",
   "metadata": {},
   "source": [
    "## Run the experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "abe58618",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Replay Buffer Load file empty\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                            | 0/1 [00:01<?, ?it/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "object of type 'NoneType' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-f05b480a7c43>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     77\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     78\u001b[0m \u001b[1;31m# run experiment\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 79\u001b[1;33m \u001b[0mrun_experiment\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcurrent_env\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcurrent_agent\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0menvironment_parameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0magent_parameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexperiment_parameters\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-9-f05b480a7c43>\u001b[0m in \u001b[0;36mrun_experiment\u001b[1;34m(environment, agent, environment_parameters, agent_parameters, experiment_parameters)\u001b[0m\n\u001b[0;32m     24\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mepisode\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexperiment_parameters\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"num_episodes\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m             \u001b[1;31m# run episode\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m             \u001b[0mrl_glue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrl_episode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexperiment_parameters\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"timeout\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     27\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m             \u001b[0mepisode_reward\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrl_glue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrl_agent_message\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"get_sum_reward\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\Github\\OpenAIGymReinfLearning\\OpenAIGym_AtariBreakout\\rl_glue.py\u001b[0m in \u001b[0;36mrl_episode\u001b[1;34m(self, max_steps_this_episode)\u001b[0m\n\u001b[0;32m    213\u001b[0m         while (not is_terminal) and ((max_steps_this_episode == 0) or\n\u001b[0;32m    214\u001b[0m                                      (self.num_steps < max_steps_this_episode)):\n\u001b[1;32m--> 215\u001b[1;33m             \u001b[0mrl_step_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrl_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    216\u001b[0m             \u001b[0mis_terminal\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrl_step_result\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    217\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtotal_reward\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\Github\\OpenAIGymReinfLearning\\OpenAIGym_AtariBreakout\\rl_glue.py\u001b[0m in \u001b[0;36mrl_step\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    141\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    142\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_steps\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 143\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlast_action\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0magent_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlast_state\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    144\u001b[0m             \u001b[0mroat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlast_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlast_action\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mterm\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    145\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-7-37117f47cc70>\u001b[0m in \u001b[0;36magent_step\u001b[1;34m(self, reward, state)\u001b[0m\n\u001b[0;32m     94\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     95\u001b[0m         \u001b[1;31m# Append new experience to replay buffer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 96\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreplay_buffer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlast_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlast_action\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     97\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     98\u001b[0m         \u001b[1;31m# Perform replay steps:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-3-158c31cd9f7c>\u001b[0m in \u001b[0;36mappend\u001b[1;34m(self, state, action, reward, terminal, next_state)\u001b[0m\n\u001b[0;32m     23\u001b[0m             \u001b[0mnext_state\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mNumpy\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mThe\u001b[0m \u001b[0mnext\u001b[0m \u001b[0mstate\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m         \"\"\"\n\u001b[1;32m---> 25\u001b[1;33m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_size\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m             \u001b[1;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m         \u001b[0mstate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstate\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'float32'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: object of type 'NoneType' has no len()"
     ]
    }
   ],
   "source": [
    "\n",
    "def run_experiment(environment, agent, environment_parameters, agent_parameters, experiment_parameters):\n",
    "    \n",
    "    rl_glue = RLGlue(environment, agent)\n",
    "        \n",
    "    # save sum of reward at the end of each episode\n",
    "    agent_sum_reward = np.zeros((experiment_parameters[\"num_runs\"], \n",
    "                                 experiment_parameters[\"num_episodes\"]))\n",
    "    trained_agent_sum_reward = np.zeros((experiment_parameters[\"num_runs\"], \n",
    "                                 experiment_parameters[\"num_episodes_trained\"]))\n",
    "\n",
    "    env_info = {}\n",
    "\n",
    "    agent_info = agent_parameters\n",
    "    \n",
    "    # one agent setting\n",
    "    for run in range(1, experiment_parameters[\"num_runs\"]+1):\n",
    "        agent_info[\"seed\"] = run\n",
    "        agent_info[\"network_config\"][\"seed\"] = run\n",
    "        env_info[\"seed\"] = run\n",
    "\n",
    "        rl_glue.rl_init(agent_info, env_info)\n",
    "        \n",
    "            \n",
    "        for episode in tqdm(range(1, experiment_parameters[\"num_episodes\"]+1)):\n",
    "            # run episode\n",
    "            rl_glue.rl_episode(experiment_parameters[\"timeout\"])\n",
    "            \n",
    "            episode_reward = rl_glue.rl_agent_message(\"get_sum_reward\")\n",
    "            agent_sum_reward[run - 1, episode - 1] = episode_reward\n",
    "            \n",
    "        \n",
    "        for episode in range(1, experiment_parameters[\"num_episodes_trained\"]+1):\n",
    "            rl_glue.trained_rl_episode(experiment_parameters[\"timeout\"])\n",
    "            trained_agent_sum_reward[run - 1, episode - 1 ] =  rl_glue.rl_env_message(\"get_sum_reward\")\n",
    "            \n",
    "            print(\"Trained reward = \" + str(trained_agent_sum_reward))\n",
    "    save_name = \"{}\".format(rl_glue.agent.name)\n",
    "    if not os.path.exists('results'):\n",
    "        os.makedirs('results')\n",
    "    np.save(\"results/sum_reward_{}\".format(save_name), agent_sum_reward)\n",
    "    shutil.make_archive('results', 'zip', 'results')\n",
    "    save_weights_to_json(rl_glue.agent.network.model, NAME_Save, \"weights/\")\n",
    "    \n",
    "# Run Experiment\n",
    "# tf.debugging.set_log_device_placement(True)\n",
    "# Experiment parameters\n",
    "experiment_parameters = {\n",
    "    \"num_runs\" : 1,\n",
    "    \"num_episodes_trained\" : 0,\n",
    "    \"num_episodes\" : 1,\n",
    "    # OpenAI Gym environments allow for a timestep limit timeout, causing episodes to end after \n",
    "    # some number of timesteps. Here we use the default of 500.\n",
    "    \"timeout\" : 500\n",
    "}\n",
    "\n",
    "# Environment parameters\n",
    "environment_parameters = {}\n",
    "\n",
    "current_env = AtariBreakoutEnvironment\n",
    "\n",
    "# Agent parameters\n",
    "agent_parameters = {\n",
    "    'network_config': {\n",
    "        'state_dim': [210,160,3],\n",
    "        'num_actions': 4,\n",
    "        'learning_rate':0.001\n",
    "    },\n",
    "    'replay_buffer_size': 5000,\n",
    "    'minibatch_sz': 4,\n",
    "    'dirc' : \"ReplayBuffer\",\n",
    "    'loadBool' : True,\n",
    "    'num_replay_updates_per_step': 4,\n",
    "    'gamma': 0.99,\n",
    "    'tau': 0.001\n",
    "}\n",
    "current_agent = Agent\n",
    "\n",
    "# run experiment\n",
    "run_experiment(current_env, current_agent, environment_parameters, agent_parameters, experiment_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "604a3286",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_result([\"expected_sarsa_agent\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57d98a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.load('results/sum_reward_expected_sarsa_agent.npy') \n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7683765f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
