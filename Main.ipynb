{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c881f145",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from rl_glue import RLGlue\n",
    "\n",
    "from environment import BaseEnvironment\n",
    "from atari_breakout import AtariBreakoutEnvironment\n",
    "from agent import BaseAgent \n",
    "\n",
    "from collections import deque\n",
    "from copy import deepcopy\n",
    "from tqdm import tqdm\n",
    "import os \n",
    "import time\n",
    "import shutil\n",
    "from plot_script import plot_result\n",
    "import pickle\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential, model_from_json\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a5ddb28",
   "metadata": {},
   "source": [
    "## Neural Network for action values\n",
    "\n",
    "We use a neural network with one hidden layer for approximating the action-value function in a control problem. The output layer size is the number of actions. \n",
    "\n",
    "The get_action_values() function computes the action-value function by doing a forward pass.\n",
    "The get_TD_update() function computes the gradient of the action-value function with respect to the weights times the TD error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "42e83475",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ActionValueNetwork:\n",
    "\n",
    "    def __init__(self, network_config):\n",
    "        self.state_dim = network_config.get(\"state_dim\")\n",
    "        self.num_actions = network_config.get(\"num_actions\")\n",
    "        self.learning_rate = network_config.get(\"learning_rate\")\n",
    "        \n",
    "#         inputs = layers.Input(shape=(self.state_dim, self.state_dim, 3))\n",
    "#         # Convolutions on the frames on the screen\n",
    "#         layer1 = layers.Conv2D(6, 7, strides=4, activation=\"relu\")(inputs)\n",
    "#         layer2 = layers.MaxPooling2D(pool_size = (2,2))(layer1)\n",
    "#         layer3 = layers.Conv2D(12, 4, strides=2, activation=\"relu\")(layer2)\n",
    "#         layer4 = layers.MaxPooling2D(pool_size = (2,2))(layer3)\n",
    "#         layer5 = layers.Flatten()(layer4)\n",
    "#         layer6 = layers.Dense(512, activation=\"relu\")(layer5)\n",
    "#         action = layers.Dense(self.num_actions, activation=\"linear\")(layer6)\n",
    "\n",
    "#         self.model =  keras.Model(inputs=inputs, outputs=action)\n",
    "        \n",
    "        model = Sequential()\n",
    "#         model.add(layers.Input(shape=(self.state_dim, self.state_dim, 3)))\n",
    "        model.add(layers.Conv2D(filters = 32, kernel_size = 8, strides = 4, activation=\"relu\", input_shape = (self.state_dim[0], self.state_dim[1], self.state_dim[2])))\n",
    "        \n",
    "        model.add(layers.MaxPooling2D(pool_size = (2,2), padding = 'same'))\n",
    "        model.add(layers.Conv2D(filters = 64, kernel_size = 4, strides = 3, activation=\"relu\"))\n",
    "        model.add(layers.MaxPooling2D(pool_size = (2,2), padding = 'same'))\n",
    "        model.add(layers.Conv2D(filters = 64, kernel_size = 3, strides = 1, activation=\"relu\"))\n",
    "        model.add(layers.MaxPooling2D(pool_size = (2,2), padding = 'same'))\n",
    "        model.add(layers.Flatten())\n",
    "        model.add(layers.Dense(512, activation=\"relu\"))\n",
    "        model.add(layers.Dense(self.num_actions, activation=None))\n",
    "              \n",
    "        model.compile(loss = 'mean_squared_error', optimizer=Adam(lr=self.learning_rate, epsilon=1e-7))\n",
    "        self.model = model\n",
    "#         model.summary()\n",
    "#         model = models.Sequential()\n",
    "#         model.add(layers.Conv2D(32, 8, strides = 4, activation='relu', input_shape=(32, 32, 3)))\n",
    "#         model.add(layers.MaxPooling2D((2, 2)))\n",
    "#         model.add(layers.Conv2D(64, 4, strides = 2, activation='relu'))\n",
    "#         model.add(layers.MaxPooling2D((2, 2)))\n",
    "#         model.add(layers.Conv2D(64, 3, activation='relu'))\n",
    "#         model.add(layers.Flatten())\n",
    "#         model.add(layers.Dense(512, activation=\"relu\")(layer4))\n",
    "#         model.add(layers.Dense(self.num_actions, activation=\"linear\"))\n",
    "        \n",
    "#         self.model = model\n",
    "\n",
    "    def get_action_values(self, s):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            s (Numpy array): The state.\n",
    "        Returns:\n",
    "            The action-values (Numpy array) calculated using the network's weights.\n",
    "        \"\"\"\n",
    "        \n",
    "        q_vals = self.model.predict(s)\n",
    "\n",
    "        return q_vals\n",
    "    \n",
    "\n",
    "#     def get_TD_update(self, s, delta_mat):\n",
    "#         \"\"\"\n",
    "#         Args:\n",
    "#             s (Numpy array): The state.\n",
    "#             delta_mat (Numpy array): A 2D array of shape (batch_size, num_actions). Each row of delta_mat  \n",
    "#             correspond to one state in the batch. Each row has only one non-zero element \n",
    "#             which is the TD-error corresponding to the action taken.\n",
    "#         Returns:\n",
    "#             The TD update (Array of dictionaries with gradient times TD errors) for the network's weights\n",
    "#         \"\"\"\n",
    "\n",
    "#         W0, b0 = self.weights[0]['W'], self.weights[0]['b']\n",
    "#         W1, b1 = self.weights[1]['W'], self.weights[1]['b']\n",
    "        \n",
    "#         psi = np.dot(s, W0) + b0\n",
    "#         x = np.maximum(psi, 0)\n",
    "#         dx = (psi > 0).astype(float)\n",
    "\n",
    "#         # td_update has the same structure as self.weights, that is an array of dictionaries.\n",
    "#         # td_update[0][\"W\"], td_update[0][\"b\"], td_update[1][\"W\"], and td_update[1][\"b\"] have the same shape as \n",
    "#         # self.weights[0][\"W\"], self.weights[0][\"b\"], self.weights[1][\"W\"], and self.weights[1][\"b\"] respectively\n",
    "#         td_update = [dict() for i in range(len(self.weights))]\n",
    "         \n",
    "#         v = delta_mat\n",
    "#         td_update[1]['W'] = np.dot(x.T, v) * 1. / s.shape[0]\n",
    "#         td_update[1]['b'] = np.sum(v, axis=0, keepdims=True) * 1. / s.shape[0]\n",
    "        \n",
    "#         v = np.dot(v, W1.T) * dx\n",
    "#         td_update[0]['W'] = np.dot(s.T, v) * 1. / s.shape[0]\n",
    "#         td_update[0]['b'] = np.sum(v, axis=0, keepdims=True) * 1. / s.shape[0]\n",
    "                \n",
    "#         return td_update\n",
    "        \n",
    "    \n",
    "    def get_weights(self):\n",
    "        return self.model.get_weights()\n",
    "    \n",
    "    def set_weights(self, weights):\n",
    "        self.model.set_weights(weights)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db59e3e5",
   "metadata": {},
   "source": [
    "## Replay Buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1447670b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, size, minibatch_size, seed, load_bool, file_path):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            size (integer): The size of the replay buffer.              \n",
    "            minibatch_size (integer): The sample size.\n",
    "            seed (integer): The seed for the random number generator. \n",
    "        \"\"\"\n",
    "        self.buffer = []\n",
    "        self.minibatch_size = minibatch_size\n",
    "        self.rand_generator = np.random.RandomState(seed)\n",
    "        self.max_size = size\n",
    "        self.path = file_path\n",
    "        \n",
    "        self.load_bool = load_bool\n",
    "        if self.load_bool:\n",
    "            self.load()\n",
    "    def append(self, state, action, reward, terminal, next_state):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            state (Numpy array): The state.              \n",
    "            action (integer): The action.\n",
    "            reward (float): The reward.\n",
    "            terminal (integer): 1 if the next state is a terminal state and 0 otherwise.\n",
    "            next_state (Numpy array): The next state.           \n",
    "        \"\"\"\n",
    "        if len(self.buffer) == self.max_size:\n",
    "            del self.buffer[0]\n",
    "        state = state.astype('float32')\n",
    "        next_state = next_state.astype('float32')\n",
    "        self.buffer.append([state, action, reward, terminal, next_state])\n",
    "\n",
    "    def sample(self):\n",
    "        \"\"\"\n",
    "        Returns:\n",
    "            A list of transition tuples including state, action, reward, terinal, and next_state\n",
    "        \"\"\"\n",
    "        idxs = self.rand_generator.choice(np.arange(len(self.buffer)), size=self.minibatch_size)\n",
    "        return [self.buffer[idx] for idx in idxs]\n",
    "\n",
    "    def size(self):\n",
    "        return len(self.buffer)\n",
    "    \n",
    "    def save(self):\n",
    "        \n",
    "        with open(self.path, \"wb\") as fp:   #Pickling\n",
    "            pickle.dump(self.buffer, fp)\n",
    "        print(\"Replay Buffer saved : \" + str(len(self.buffer)) )\n",
    "        \n",
    "    def load(self):\n",
    "        try:\n",
    "            path = self.path + \".txt\"\n",
    "            with open(path, \"rb\") as fp:   # Unpickling\n",
    "                self.buffer = pickle.load(fp)\n",
    "        \n",
    "            print(\"Replay Buffer loaded : \" + str(len(self.buffer)))\n",
    "        except EOFError:\n",
    "            \n",
    "            print(\"Replay Buffer Load file empty\")\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e25a8337",
   "metadata": {},
   "source": [
    "## Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fb87db71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(action_values, tau=1.0):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        action_values (Numpy array): A 2D array of shape (batch_size, num_actions). \n",
    "                       The action-values computed by an action-value network.              \n",
    "        tau (float): The temperature parameter scalar.\n",
    "    Returns:\n",
    "        A 2D array of shape (batch_size, num_actions). Where each column is a probability distribution over\n",
    "        the actions representing the policy.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Compute the preferences by dividing the action-values by the temperature parameter tau\n",
    "    preferences = action_values/tau\n",
    "    # Compute the maximum preference across the actions\n",
    "    max_preference = np.max(preferences, axis=1)\n",
    "        \n",
    "    \n",
    "    # Reshape max_preference array which has shape [Batch,] to [Batch, 1]. This allows NumPy broadcasting \n",
    "    # when subtracting the maximum preference from the preference of each action.\n",
    "    reshaped_max_preference = max_preference.reshape((-1, 1))\n",
    "    \n",
    "    # Compute the numerator, i.e., the exponential of the preference - the max preference.\n",
    "    exp_preferences = np.exp(preferences - reshaped_max_preference)\n",
    "    # Compute the denominator, i.e., the sum over the numerator along the actions axis.\n",
    "    sum_of_exp_preferences = np.sum(exp_preferences, axis=1)\n",
    "    \n",
    "   \n",
    "    \n",
    "    # Reshape sum_of_exp_preferences array which has shape [Batch,] to [Batch, 1] to  allow for NumPy broadcasting \n",
    "    # when dividing the numerator by the denominator.\n",
    "    reshaped_sum_of_exp_preferences = sum_of_exp_preferences.reshape((-1, 1))\n",
    "    \n",
    "    # Compute the action probabilities according to the equation in the previous cell.\n",
    "    action_probs = exp_preferences/reshaped_sum_of_exp_preferences\n",
    "    \n",
    "   \n",
    "    \n",
    "    \n",
    "    # squeeze() removes any singleton dimensions. It is used here because this function is used in the \n",
    "    # agent policy when selecting an action (for which the batch dimension is 1.) As np.random.choice is used in \n",
    "    # the agent policy and it expects 1D arrays, we need to remove this singleton batch dimension.\n",
    "    action_probs = action_probs.squeeze()\n",
    "    return action_probs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0635bd91",
   "metadata": {},
   "source": [
    "## Compiling\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a50cccb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_td_error(states, next_states, actions, rewards, discount, terminals, network, current_q, tau):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        states (Numpy array): The batch of states with the shape (batch_size, state_dim).\n",
    "        next_states (Numpy array): The batch of next states with the shape (batch_size, state_dim).\n",
    "        actions (Numpy array): The batch of actions with the shape (batch_size,).\n",
    "        rewards (Numpy array): The batch of rewards with the shape (batch_size,).\n",
    "        discount (float): The discount factor.\n",
    "        terminals (Numpy array): The batch of terminals with the shape (batch_size,).\n",
    "        network (ActionValueNetwork): The latest state of the network that is getting replay updates.\n",
    "        current_q (ActionValueNetwork): The fixed network used for computing the targets, \n",
    "                                        and particularly, the action-values at the next-states.\n",
    "    Returns:\n",
    "        The TD errors (Numpy array) for actions taken, of shape (batch_size,)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Here network is the latest state of the network that is getting replay updates. In other words, \n",
    "    # the network represents Q_{t+1}^{i} whereas current_q represents Q_t, the fixed network used for computing the \n",
    "    # targets, and particularly, the action-values at the next-states.\n",
    "    \n",
    "    # Compute action values at next states using current_q network\n",
    "    # q_next_mat is a 2D array of shape (batch_size, num_actions)\n",
    "    \n",
    "    # Q(t+1)\n",
    "    q_next_mat = current_q.get_action_values(next_states)\n",
    " \n",
    "    # Compute policy at next state by passing the action-values in q_next_mat to softmax()\n",
    "    # probs_mat is a 2D array of shape (batch_size, num_actions)\n",
    "    \n",
    "    # Pi\n",
    "    probs_mat = softmax(q_next_mat, tau)\n",
    "\n",
    "    # Compute the estimate of the next state value, v_next_vec.\n",
    "    # v_next_vec is a 1D array of shape (batch_size,)\n",
    "\n",
    "    v_next_vec = np.sum(probs_mat * q_next_mat, axis=1) * (1-terminals)\n",
    "    \n",
    "    # Compute Expected Sarsa target\n",
    "    # target_vec is a 1D array of shape (batch_size,)\n",
    "    \n",
    "    # \n",
    "    target_vec = rewards + discount*v_next_vec\n",
    "\n",
    "    # Compute action values at the current states for all actions using network\n",
    "    # q_mat is a 2D array of shape (batch_size, num_actions)\n",
    "   \n",
    "    q_mat = network.get_action_values(states)\n",
    "\n",
    "    # Batch Indices is an array from 0 to the batch size - 1. \n",
    "    batch_indices = np.arange(q_mat.shape[0])\n",
    "\n",
    "    # Compute q_vec by selecting q(s, a) from q_mat for taken actions\n",
    "    # Use batch_indices as the index for the first dimension of q_mat\n",
    "    # q_vec is a 1D array of shape (batch_size)\n",
    "    \n",
    "#     q_vec = q_mat[batch_indices, int(actions)]\n",
    "\n",
    "    # Compute TD errors for actions taken\n",
    "    # delta_vec is a 1D array of shape (batch_size)\n",
    "    \n",
    "#     delta_vec = target_vec - q_vec\n",
    "    target = np.zeros(q_next_mat.shape)\n",
    "    for i in range(q_next_mat.shape[0]):\n",
    "        \n",
    "        target[i,int(actions[i])] = target_vec[i]\n",
    "        \n",
    "    return target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "43dcf7b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_network(experiences, discount, optimizer, network, current_q, tau):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        experiences (Numpy array): The batch of experiences including the states, actions, \n",
    "                                   rewards, terminals, and next_states.\n",
    "        discount (float): The discount factor.\n",
    "        network (ActionValueNetwork): The latest state of the network that is getting replay updates.\n",
    "        current_q (ActionValueNetwork): The fixed network used for computing the targets, \n",
    "                                        and particularly, the action-values at the next-states.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get states, action, rewards, terminals, and next_states from experiences\n",
    "    states, actions, rewards, terminals, next_states = map(list, zip(*experiences))\n",
    "    states = np.concatenate(states)\n",
    "    next_states = np.concatenate(next_states)\n",
    "    rewards = np.array(rewards)\n",
    "    terminals = np.array(terminals)\n",
    "    batch_size = states.shape[0]\n",
    "    loss_function = keras.losses.Huber()\n",
    "    # Compute TD error using the get_td_error function\n",
    "    # q_vec is a 1D array of shape (batch_size)\n",
    "    target_vec = get_td_error(states, next_states, actions, rewards, discount, terminals, network, current_q, tau)\n",
    "#     tensorboard = TensorBoard(log_dir=\"logs/{}\".format(NAME))\n",
    "    network.model.fit(states, target_vec, epochs=1, verbose=0)\n",
    "#     callbacks=[tensorboard]\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "003c90e3",
   "metadata": {},
   "source": [
    "## Agent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "60c28d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent(BaseAgent):\n",
    "    def __init__(self):\n",
    "        self.name = \"expected_sarsa_agent\"\n",
    "        self.network = None\n",
    "    def agent_init(self, agent_config):\n",
    "        \"\"\"Setup for the agent called when the experiment first starts.\n",
    "\n",
    "        Set parameters needed to setup the agent.\n",
    "\n",
    "        Assume agent_config dict contains:\n",
    "        {\n",
    "            network_config: dictionary,\n",
    "            optimizer_config: dictionary,\n",
    "            replay_buffer_size: integer,\n",
    "            minibatch_sz: integer, \n",
    "            num_replay_updates_per_step: float\n",
    "            discount_factor: float,\n",
    "        }\n",
    "        \"\"\"\n",
    "        self.replay_buffer = ReplayBuffer(agent_config['replay_buffer_size'], \n",
    "                                          agent_config['minibatch_sz'], agent_config.get(\"seed\"),  agent_config[\"loadReplayBuffer\"], agent_config[\"replay_file_path\"])\n",
    "        self.network = ActionValueNetwork(agent_config['network_config'])\n",
    "        self.network_target = ActionValueNetwork(agent_config['network_config'])\n",
    "        self.num_actions = agent_config['network_config']['num_actions']\n",
    "        self.num_replay = agent_config['num_replay_updates_per_step']\n",
    "        self.discount = agent_config['gamma']\n",
    "        self.tau = agent_config['tau']\n",
    "        \n",
    "        self.rand_generator = np.random.RandomState(agent_config.get(\"seed\"))\n",
    "        self.epsilon = 0.01\n",
    "        self.last_state = None\n",
    "        self.last_action = None\n",
    "        self.sum_rewards = 0\n",
    "        self.episode_steps = 0\n",
    "        self.optimizer = keras.optimizers.Adam(learning_rate=0.00025, clipnorm=1.0)\n",
    "        self.action_space = np.array([i for i in range(self.num_actions)])\n",
    "        self.action_space = self.action_space.astype('float32')\n",
    "        self.eps = 1\n",
    "        self.eps_min = 0.1\n",
    "        self.eps_decay = 0.999\n",
    "        \n",
    "            \n",
    "            \n",
    "    def save_model(self, path):\n",
    "        self.network.model.save(path)\n",
    "        print(\"Model saved to : \" + str(path))\n",
    "    def load_model(self, path):\n",
    "        self.network.model = keras.models.load_model(path)\n",
    "        print(\"Model loaded from : \" + str(path))\n",
    "        \n",
    "    def policy(self, state):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            state (Numpy array): the state.\n",
    "        Returns:\n",
    "            the action. \n",
    "        \"\"\"\n",
    "        if np.random.rand() > self.epsilon:\n",
    "            \n",
    "            action_values = self.network.get_action_values(state)\n",
    "            probs_batch = softmax(action_values, self.tau)\n",
    "            action = self.rand_generator.choice(self.num_actions, p=probs_batch.squeeze())\n",
    "        else:\n",
    "            action = np.random.choice(self.action_space)\n",
    "        return action\n",
    "\n",
    "    def agent_start(self, state):\n",
    "        \"\"\"The first method called when the experiment starts, called after\n",
    "        the environment starts.\n",
    "        Args:\n",
    "            state (Numpy array): the state from the\n",
    "                environment's evn_start function.\n",
    "        Returns:\n",
    "            The first action the agent takes.\n",
    "        \"\"\"\n",
    "        self.sum_rewards = 0\n",
    "        self.episode_steps = 0\n",
    "        self.last_state = np.array([state])\n",
    "        self.last_action = self.policy(self.last_state)\n",
    "        return self.last_action\n",
    "\n",
    "\n",
    "    def agent_step(self, reward, state):\n",
    "        \"\"\"A step taken by the agent.\n",
    "        Args:\n",
    "            reward (float): the reward received for taking the last action taken\n",
    "            state (Numpy array): the state from the\n",
    "                environment's step based, where the agent ended up after the\n",
    "                last step\n",
    "        Returns:\n",
    "            The action the agent is taking.\n",
    "        \"\"\"\n",
    "        \n",
    "        self.sum_rewards += reward\n",
    "        self.episode_steps += 1\n",
    "        \n",
    "        # Make state an array of shape (1, state_dim) to add a batch dimension and\n",
    "        # to later match the get_action_values() and get_TD_update() functions\n",
    "        state = np.array([state])\n",
    "\n",
    "        # Select action\n",
    "        action = self.policy(state)\n",
    "        \n",
    "        # Append new experience to replay buffer\n",
    "        self.replay_buffer.append(self.last_state, self.last_action, reward, 0, state)\n",
    "        \n",
    "        # Perform replay steps:\n",
    "        if self.replay_buffer.size() > self.replay_buffer.minibatch_size:\n",
    "#             current_q = deepcopy(self.network)\n",
    "            self.network_target.set_weights(self.network.get_weights())\n",
    "            for _ in range(self.num_replay):\n",
    "                \n",
    "                # Get sample experiences from the replay buffer\n",
    "                experiences = self.replay_buffer.sample()\n",
    "                \n",
    "                # Call optimize_network to update the weights of the network \n",
    "                optimize_network(experiences, self.discount, self.optimizer, self.network, self.network_target, self.tau)\n",
    "                \n",
    "        # Update the last state and last action.\n",
    "        self.last_state = state\n",
    "        self.last_action = action\n",
    "        \n",
    "        if self.eps > self.eps_min:\n",
    "            self.eps *= self.eps_decay\n",
    "        \n",
    "        \n",
    "        return action\n",
    "\n",
    "    def agent_end(self, reward):\n",
    "        \"\"\"Run when the agent terminates.\n",
    "        Args:\n",
    "            reward (float): the reward the agent received for entering the\n",
    "                terminal state.\n",
    "        \"\"\"\n",
    "        self.sum_rewards += reward\n",
    "        self.episode_steps += 1\n",
    "        \n",
    "        # Set terminal state to an array of zeros\n",
    "        state = np.zeros_like(self.last_state)\n",
    "\n",
    "        # Append new experience to replay buffer       \n",
    "       \n",
    "        self.replay_buffer.append(self.last_state, self.last_action, reward, 1, state)\n",
    "        \n",
    "        # Perform replay steps:\n",
    "        if self.replay_buffer.size() > self.replay_buffer.minibatch_size:\n",
    "#             current_q = deepcopy(self.network)\n",
    "            self.network_target.set_weights(self.network.get_weights())\n",
    "            for _ in range(self.num_replay):\n",
    "                \n",
    "                # Get sample experiences from the replay buffer\n",
    "                experiences = self.replay_buffer.sample()\n",
    "                \n",
    "                # Call optimize_network to update the weights of the network\n",
    "                optimize_network(experiences, self.discount, self.optimizer, self.network, self.network_target, self.tau)\n",
    "                \n",
    "        \n",
    "    def trained_agent_step(self, reward, state):\n",
    "        \"\"\"A step taken by the trained agent.\n",
    "        Args:\n",
    "            reward (float): the reward received for taking the last action taken\n",
    "            state (Numpy array): the state from the\n",
    "                environment's step based, where the agent ended up after the\n",
    "                last step\n",
    "        Returns:\n",
    "            The action the agent is taking.\n",
    "        \"\"\"\n",
    "        \n",
    "        \n",
    "        self.sum_rewards += reward\n",
    "        # Make state an array of shape (1, state_dim) to add a batch dimension and\n",
    "        # to later match the get_action_values() and get_TD_update() functions\n",
    "        state = np.array([state])\n",
    "\n",
    "        # Select action\n",
    "        action = self.policy(state)\n",
    "        \n",
    "        return action\n",
    "    \n",
    "    def agent_message(self, message):\n",
    "        if message == \"get_sum_reward\":\n",
    "            return self.sum_rewards\n",
    "        else:\n",
    "            raise Exception(\"Unrecognized Message!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fbbbe316",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_weights_model(model, file_name, path):\n",
    "\n",
    "    path = os.path.join(path, file_name)\n",
    "    model.save(path)\n",
    "\n",
    "    print(\"Saved model to disk\")\n",
    "    \n",
    "def load_weights_model(model, file_name, path):\n",
    "\n",
    "    path = os.path.join(path, file_name)\n",
    "    model = keras.models.load_model('path')\n",
    "\n",
    "    print(\"Loaded model from disk\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21f250b0",
   "metadata": {},
   "source": [
    "## Run the experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abe58618",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Replay Buffer loaded : 1460\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                            | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded from : weights\\CNN-32-64-64-512-1628469665\n"
     ]
    }
   ],
   "source": [
    "model_save_name = \"CNN-32-64-64-512-{}\".format(int(time.time()))\n",
    "model_load_name = \"CNN-32-64-64-512-1628469665\"\n",
    "\n",
    "\n",
    "def run_experiment(environment, agent, environment_parameters, agent_parameters, experiment_parameters):\n",
    "    \n",
    "    rl_glue = RLGlue(environment, agent)\n",
    "        \n",
    "    # save sum of reward at the end of each episode\n",
    "    agent_sum_reward = np.zeros((experiment_parameters[\"num_runs\"], \n",
    "                                 experiment_parameters[\"num_episodes\"]))\n",
    "    trained_agent_sum_reward = np.zeros((experiment_parameters[\"num_runs\"], \n",
    "                                 experiment_parameters[\"num_episodes_trained\"]))\n",
    "\n",
    "    env_info = {}\n",
    "\n",
    "    agent_info = agent_parameters\n",
    "    \n",
    "    # one agent setting\n",
    "    for run in range(1, experiment_parameters[\"num_runs\"]+1):\n",
    "        agent_info[\"seed\"] = run\n",
    "        agent_info[\"network_config\"][\"seed\"] = run\n",
    "        env_info[\"seed\"] = run\n",
    "\n",
    "        rl_glue.rl_init(agent_info, env_info)\n",
    "        \n",
    "#         Load model from disk\n",
    "        rl_glue.agent.load_model(os.path.join(\"weights\", model_load_name))\n",
    "        \n",
    "        for episode in tqdm(range(1, experiment_parameters[\"num_episodes\"]+1)):\n",
    "            # run episode\n",
    "            rl_glue.rl_episode(experiment_parameters[\"timeout\"])\n",
    "            \n",
    "            episode_reward = rl_glue.rl_agent_message(\"get_sum_reward\")\n",
    "            agent_sum_reward[run - 1, episode - 1] = episode_reward\n",
    "            \n",
    "        \n",
    "        for episode in range(1, experiment_parameters[\"num_episodes_trained\"]+1):\n",
    "            rl_glue.trained_rl_episode(experiment_parameters[\"timeout\"])\n",
    "            trained_agent_sum_reward[run - 1, episode - 1 ] =  rl_glue.rl_env_message(\"get_sum_reward\")\n",
    "            \n",
    "            print(\"Trained reward = \" + str(trained_agent_sum_reward))\n",
    "            \n",
    "#     Save Reward data\n",
    "    \n",
    "    if not os.path.exists(\"results/sum_reward_{}.npy\".format(model_load_name)):\n",
    "        \n",
    "        np.save(\"results/sum_reward_{}\".format(model_save_name), agent_sum_reward)\n",
    "    else:\n",
    "        \n",
    "        a = np.load(\"results/sum_reward_{}.npy\".format(model_load_name))\n",
    "        print(\"results loaded \")\n",
    "        a = np.append(a, agent_sum_reward, axis = 1)\n",
    "        np.save(\"results/sum_reward_{}\".format(model_save_name), a)\n",
    "    shutil.make_archive('results', 'zip', 'results')\n",
    "    \n",
    "    rl_glue.agent.save_model(os.path.join(\"weights\", model_save_name))\n",
    "    \n",
    "    rl_glue.agent.replay_buffer.save()\n",
    "    \n",
    "# Run Experiment\n",
    "# tf.debugging.set_log_device_placement(True)\n",
    "# Experiment parameters\n",
    "experiment_parameters = {\n",
    "    \"num_runs\" : 1,\n",
    "    \"num_episodes_trained\" : 0,\n",
    "    \"num_episodes\" : 1,\n",
    "    # OpenAI Gym environments allow for a timestep limit timeout, causing episodes to end after \n",
    "    # some number of timesteps. Here we use the default of 500.\n",
    "    \"timeout\" : 500\n",
    "}\n",
    "\n",
    "# Environment parameters\n",
    "environment_parameters = {}\n",
    "\n",
    "current_env = AtariBreakoutEnvironment\n",
    "\n",
    "\n",
    "\n",
    "# Agent parameters\n",
    "agent_parameters = {\n",
    "    'network_config': {\n",
    "        'state_dim': [210,160,3],\n",
    "        'num_actions': 4,\n",
    "        'learning_rate':0.001\n",
    "    },\n",
    "    'replay_buffer_size': 5000,\n",
    "    'minibatch_sz': 4,\n",
    "    'replay_file_path' : os.path.join(\"ReplayBuffer\", \"replay_buffer_1\"),\n",
    "    'loadReplayBuffer' : True,\n",
    "    'num_replay_updates_per_step': 4,\n",
    "    'gamma': 0.99,\n",
    "    'tau': 0.001\n",
    "}\n",
    "current_agent = Agent\n",
    "\n",
    "# run experiment\n",
    "run_experiment(current_env, current_agent, environment_parameters, agent_parameters, experiment_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "604a3286",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_result([\"expected_sarsa_agent\"], \"CNN-32-64-64-512-1628471546\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57d98a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.load(\"results/sum_reward_{}.npy\".format(model_save_name)) \n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7683765f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
